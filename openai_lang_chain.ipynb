{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGdRJS6vOVcA+Ynx2NpFwk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moulaali/colabs/blob/main/openai_lang_chain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dependencies"
      ],
      "metadata": {
        "id": "zkMvTjdWNof4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install langchain_decorators"
      ],
      "metadata": {
        "id": "u0OCBtUtayhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Auth Setup"
      ],
      "metadata": {
        "id": "u4dR-Xx7NlYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "yobdfTPIbHQ-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checker Chain : Self fact checking"
      ],
      "metadata": {
        "id": "WF66vy3f90P2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "JPqsbuj_aXXY",
        "outputId": "3a00fd6e-31df-4549-9f12-c691afdfc4de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMCheckerChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The type of mammal that lays the biggest eggs is the platypus, which is a type of monotreme.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Interestingly, this returns different results every time\n",
        "from langchain.chains import LLMCheckerChain\n",
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(temperature=0.7)\n",
        "text = \"What type of mammal lays the biggest eggs?\"\n",
        "checker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\n",
        "checker_chain.run(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization of webpage with prompt"
      ],
      "metadata": {
        "id": "cZtD7GQ9-t5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# html parsing\n",
        "!pip install requests beautifulsoup4"
      ],
      "metadata": {
        "id": "wHSstGxr_008"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_url_body(url):\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "      soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "      # Extract the body text\n",
        "      body_text = soup.body.get_text(separator=' ', strip=True)\n",
        "\n",
        "      return body_text\n",
        "  else:\n",
        "      print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
        "      return \"failure\""
      ],
      "metadata": {
        "id": "GyUvPCqO__Nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_txt = get_url_body(url=\"https://abcnews.go.com/Politics/tiktok-sues-federal-government-potential-us-ban/story?id=109994231\")\n",
        "url_txt"
      ],
      "metadata": {
        "id": "6SpWRfK9AXKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_decorators import llm_prompt\n",
        "@llm_prompt\n",
        "def summarize(text:str, length=\"short\") -> str:\n",
        "    \"\"\"\n",
        "    Summarize this text in {length} sentences:\n",
        "    {text}\n",
        "    \"\"\"\n",
        "    return"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vj6291Uk-elk",
        "outputId": "9f40d22e-5765-48fb-be09-6850b281bda8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Could not make llm client=<openai.resources.chat.completions.Completions object at 0x79fd7cc2ab90> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x79fd7cc4c2e0> model_name='gpt-3.5-turbo-1106' temperature=0.0 openai_api_key='sk-proj-jyRv9enzHWwtS3YbG2jVT3BlbkFJAtnWwJr06V4NwE6bcc82' openai_proxy='' request_timeout=30.0 streamable. Error: 'ChatOpenAI' object has no attribute '_lc_kwargs'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = summarize(text=url_txt, length=100)\n",
        "summary"
      ],
      "metadata": {
        "id": "aPGLVgoA-4Sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chain of Density"
      ],
      "metadata": {
        "id": "fitHk4igBtgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Article: {text}\n",
        "You will generate increasingly concise, entity-dense summaries of the above article.\n",
        "Repeat the following 2 steps 5 times.\n",
        "Step 1. Identify 1-3 informative entities (\";\" delimited) from the article which are missing from the previously generated summary.\n",
        "Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities.\n",
        "A missing entity is:\n",
        "- relevant to the main story,\n",
        "- specific yet concise (5 words or fewer),\n",
        "- novel (not in the previous summary),\n",
        "- faithful (present in the article),\n",
        "- anywhere (can be located anywhere in the article).\n",
        "Guidelines:\n",
        "- The first summary should be long (4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~80 words.\n",
        "- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n",
        "- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n",
        "- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the article.\n",
        "- Missing entities can appear anywhere in the new summary.\n",
        "- Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n",
        "Remember, use the exact same number of words for each summary.\n",
        "Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\n",
        "\"\"\"\n",
        "summary = llm(template.format(text=url_txt))\n",
        "summary"
      ],
      "metadata": {
        "id": "k_YAi1DWBsEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Token usage"
      ],
      "metadata": {
        "id": "UCE2-9-4dsYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, PromptTemplate\n",
        "from langchain.callbacks import get_openai_callback\n",
        "llm_chain = PromptTemplate.from_template(\"Tell me a joke about {topic}!\") | OpenAI()\n",
        "with get_openai_callback() as cb:\n",
        "    response = llm_chain.invoke(dict(topic=\"light bulbs\"))\n",
        "    print(response)\n",
        "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
        "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
        "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
        "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PS28Meddjoz",
        "outputId": "0268e72f-5044-4420-c462-97528b033d9d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Why was the light bulb feeling depressed?\n",
            "\n",
            "Because it just couldn't seem to brighten up the room!\n",
            "Total Tokens: 30\n",
            "Prompt Tokens: 8\n",
            "Completion Tokens: 22\n",
            "Total Cost (USD): $5.6e-05\n"
          ]
        }
      ]
    }
  ]
}